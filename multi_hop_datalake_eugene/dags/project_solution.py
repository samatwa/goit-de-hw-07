from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime
import os

# ÐÐ°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ Ð·Ð° Ð·Ð°Ð¼Ð¾Ð²Ñ‡ÑƒÐ²Ð°Ð½Ð½ÑÐ¼
default_args = {
    'owner': 'eugene',
    'start_date': datetime(2024, 8, 4),
}

# ðŸ” Ð¨Ð»ÑÑ… Ð´Ð¾ ÑÐºÑ€Ð¸Ð¿Ñ‚Ñ–Ð²
SCRIPTS_DIR = "/root/airflow-docker/dags/scripts"

# Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ DAG
with DAG(
    dag_id='multi_hop_datalake_eugene',
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
    tags=["eugene"],
    description='Pipeline for multi-hop datalake: landing â†’ bronze â†’ silver â†’ gold',
) as dag:

    # Landing to bronze
    landing_to_bronze_all = SparkSubmitOperator(
        application=f"{SCRIPTS_DIR}/landing_to_bronze.py",
        task_id='landing_to_bronze_all_tables',
        conn_id='spark-default',
        verbose=1,
        dag=dag
    )

    # Bronze to silver
    bronze_to_silver_all = SparkSubmitOperator(
        application=f"{SCRIPTS_DIR}/bronze_to_silver.py",
        task_id='bronze_to_silver_all_tables',
        conn_id='spark-default',
        verbose=1,
        dag=dag
    )

    # Silver to gold (Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ñ–Ñ)
    silver_to_gold_avg_stats = SparkSubmitOperator(
        application=f"{SCRIPTS_DIR}/silver_to_gold.py",
        task_id='silver_to_gold_avg_stats',
        conn_id='spark-default',
        verbose=1,
        dag=dag
    )

    # ÐŸÐ¾ÑÐ»Ñ–Ð´Ð¾Ð²Ð½Ñ–ÑÑ‚ÑŒ Ð²Ð¸ÐºÐ¾Ð½Ð°Ð½Ð½Ñ
    landing_to_bronze_all >> bronze_to_silver_all >> silver_to_gold_avg_stats
